# Pok√©mon ETL Pipeline
A comprehensive ETL pipeline that fetches Pok√©mon data from the Pok√©API, processes it, stores it in SQLite, and serves it via a RESTful API with a web interface.

![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)
![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)
![SQLite](https://img.shields.io/badge/SQLite-3-lightgrey.svg)
![Docker](https://img.shields.io/badge/Docker-Ready-blue.svg)
![License](https://img.shields.io/badge/License-MIT-yellow.svg)

---

## üìò Project Description
The **Pok√©mon ETL Pipeline** is a full-stack data engineering project that demonstrates how to build an end-to-end **Extract, Transform, Load (ETL)** system integrated with a **web API** and **interactive frontend**.

It automatically retrieves Pok√©mon data from the public **[Pok√©API](https://pokeapi.co/)**, cleans and structures it into a **normalized SQLite database**, and exposes it through a **FastAPI RESTful API** and **web interface**.

The project highlights key engineering concepts such as:
- API data extraction and rate-limiting  
- Data normalization and relational schema design  
- ETL orchestration with logging and error handling  
- RESTful API design with automatic documentation  
- Containerization using **Docker** for reproducible deployments  
- Unit testing and modular architecture  

This project is ideal for developers, data engineers, or learners who want to explore **real-world ETL pipelines**, **database design**, and **API integration** using **Python**, **FastAPI**, and **Docker**.

---

> üìù **Note:**  
> The project includes **comprehensive logging** across all ETL steps and the FastAPI backend.  
> Logs capture detailed information about extraction, transformation, loading, and API events to assist in debugging and monitoring.

---

## Features
- **Complete ETL Pipeline** ‚Äî Extract ‚Üí Transform ‚Üí Load with full logging and error handling  
- **FastAPI Backend** ‚Äî RESTful API with automatic docs and modular routing  
- **Modern Web Interface** ‚Äî Clean, browser-based UI for viewing and filtering Pok√©mon data  
- **Advanced Filters** ‚Äî Filter by type, HP, attack, and evolution status  
- **SQLite Storage** ‚Äî Lightweight, normalized relational database  
- **Dockerized Deployment** ‚Äî Easily containerized and portable  
- **Tested Codebase** ‚Äî Unit tests covering all core modules  

---

## üß© Project Structure
```

POKE/
‚îú‚îÄ‚îÄ app.py
‚îú‚îÄ‚îÄ main.py
‚îú‚îÄ‚îÄ constants.py
‚îú‚îÄ‚îÄ index.html
‚îÇ
‚îú‚îÄ‚îÄ crud/
‚îú‚îÄ‚îÄ data_processing/
‚îÇ   ‚îú‚îÄ‚îÄ extract.py
‚îÇ   ‚îú‚îÄ‚îÄ load.py
‚îÇ   ‚îî‚îÄ‚îÄ transform.py
‚îÇ
‚îú‚îÄ‚îÄ db/
‚îÇ   ‚îî‚îÄ‚îÄ pokemon_database.db
‚îÇ
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ files.py
‚îÇ   ‚îî‚îÄ‚îÄ history.py
‚îÇ
‚îú‚îÄ‚îÄ routers/
‚îú‚îÄ‚îÄ schemas/
‚îÇ   ‚îî‚îÄ‚îÄ etl.py
‚îÇ
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ **init**.py
‚îÇ   ‚îú‚îÄ‚îÄ test_extract.py
‚îÇ   ‚îú‚îÄ‚îÄ test_load.py
‚îÇ   ‚îú‚îÄ‚îÄ test_main.py
‚îÇ   ‚îî‚îÄ‚îÄ test_transform.py
‚îÇ
‚îú‚îÄ‚îÄ uploads/
‚îÇ
‚îú‚îÄ‚îÄ dockerfile
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ .dockerignore
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ requirements_test.txt
‚îú‚îÄ‚îÄ Readme.md
‚îî‚îÄ‚îÄ testing/
‚îî‚îÄ‚îÄ testing.ipynb

````

---

## ‚öôÔ∏è Prerequisites
- Python **3.10+**
- `pip` (Python package manager)
- Internet connection (for Pok√©API access)
- Optional: **Docker** (for containerized deployment)

---

## üöÄ Installation
### 1. Clone the Repository
```bash
git clone https://github.com/yourusername/pokemon-etl-pipeline.git
cd pokemon-etl-pipeline/POKE
````

### 2. Create and Activate Virtual Environment

**macOS/Linux:**

```bash
python3 -m venv venv
source venv/bin/activate
```

**Windows:**

```bash
python -m venv venv
venv\Scripts\activate
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

---

## üß± Docker Containerization

This project is fully containerized for easy deployment using **Docker**.

### üê≥ Build Docker Image

```bash
docker build -t backend-app:latest .
```

### üöÄ Run Docker Container

```bash
docker run \
  --name backend-app \
  -p 8000:8000 \
  -v "$(pwd)":/app \
  backend-app:latest
```

### üåê Access the App

* Web Interface ‚Üí [http://localhost:8000](http://localhost:8000)
* API Docs ‚Üí [http://localhost:8000/docs](http://localhost:8000/docs)
* ReDoc ‚Üí [http://localhost:8000/redoc](http://localhost:8000/redoc)

---

## üß™ Testing

```bash
python3 -m unittest discover -v
```

If you maintain separate test dependencies:

```bash
pip install -r requirements_test.txt
python3 -m unittest discover -v
```

---

## ‚öôÔ∏è Configuration

Edit `constants.py` to modify pipeline settings:

```python
POKEAPI_BASE_URL = "https://pokeapi.co/api/v2"
API_DELAY = 0.5
DATABASE_FILE = "db/pokemon_database.db"
POKEMON_TO_FETCH = 12
```

---

## üß© Design Choices (ETL, Data Mapping, Database Schema & Framework Choice )

### 1. **ETL Workflow**

The pipeline follows a clean separation of concerns:

* **Extract (`extract.py`)** ‚Äî Retrieves Pok√©mon data via the Pok√©API with rate-limiting to prevent throttling. Each Pok√©mon‚Äôs details and evolution chain are fetched independently for modularity.
* **Transform (`transform.py`)** ‚Äî Converts nested JSON from Pok√©API into structured data that fits a relational model. Complex lists (types, abilities, stats) are flattened while preserving relationships.
* **Load (`load.py`)** ‚Äî Performs idempotent inserts/updates into the SQLite database, ensuring repeated runs do not cause data duplication.

### 2. **Data Transformation**

* The raw Pok√©API response contains deeply nested structures (types, abilities, stats, evolution chains).
* To support efficient querying and filtering, the transformation step:

  * Normalizes nested lists into lookup tables (e.g., `types`, `abilities`, `stats`).
  * Links them through junction tables (`pokemon_types`, `pokemon_abilities`, `pokemon_stats`).
  * Ensures consistent naming and key mappings for downstream analytics.
* This design allows powerful queries like:

```sql
SELECT 
    p.name AS pokemon_name,
    GROUP_CONCAT(pt.type_name, ', ') AS types,
    MAX(CASE WHEN ps.stat_name = 'hp' THEN ps.base_stat END) AS hp,
    MAX(CASE WHEN ps.stat_name = 'attack' THEN ps.base_stat END) AS attack
FROM 
    pokemon p
JOIN 
    pokemon_types pt ON p.id = pt.pokemon_id
JOIN 
    pokemon_stats ps ON p.id = ps.pokemon_id
JOIN 
    stats s ON ps.stat_name = s.name
GROUP BY 
    p.name
HAVING 
    hp > 50 AND attack > 60;
```

### 3. **Database Schema Design**

* **Normalization Level:** 3NF ‚Äî Each entity (Pok√©mon, type, ability, stat, evolution) is stored in its own table, avoiding redundancy.
* **Relationship Mapping:**

  * **One-to-Many:** Evolution chains ‚Üí Pok√©mon
  * **Many-to-Many:** Pok√©mon ‚Üî Types / Abilities / Stats
* **Advantages:**

  * Improved data consistency and query performance
  * Easier updates without redundancy
  * Clean joins for data exploration or future analytics

### 4. **Idempotency & Error Handling**

* Each ETL stage is **idempotent** ‚Äî re-running it will not duplicate or corrupt data.
* Logging captures all critical events and exceptions for debugging.
* Transaction management ensures partial failures do not leave the database in an inconsistent state.

### 5. **Why SQLite**

* Lightweight, serverless, and perfect for local and demo-scale ETL pipelines.
* Can easily be swapped with PostgreSQL or MySQL by adjusting connection settings in `load.py`.

### 6. **Framework Choice ‚Äî FastAPI**

**FastAPI** was chosen as the backend framework for several important reasons:

#### üß† Why FastAPI

* **Modern and high-performance:** Built on top of **ASGI** and **Starlette**, FastAPI supports asynchronous execution, making it ideal for ETL and API-heavy workloads.
* **Automatic Documentation:** Generates **OpenAPI (Swagger)** and **ReDoc** documentation automatically ‚Äî accessible at `/docs` and `/redoc`.
* **Validation & Serialization:** Built-in **Pydantic models** ensure type-safe request/response validation, reducing runtime errors.
* **Ease of Integration:** Works seamlessly with **SQLite**, **SQLAlchemy**, and other libraries; easy to containerize with **Docker**.
* **Developer Productivity:** Extremely concise syntax and auto-generated docs make development and debugging faster.

#### üöÄ Benefits in This Project

* Used to expose RESTful endpoints like `/pokemon`, `/pokemon/filter`, and `/etl/run-pipeline`
* Allows filtering Pok√©mon data directly via query parameters (type, hp, attack, evolution status)
* Returns clean JSON responses and descriptive error messages
* Enables future scalability (e.g., migrating to PostgreSQL or deploying on Kubernetes)

---

## üèóÔ∏è Architecture

```
EXTRACT ‚Üí TRANSFORM ‚Üí LOAD
```

1. **Extract** ‚Äî Retrieve Pok√©mon data from Pok√©API
2. **Transform** ‚Äî Normalize and clean the data
3. **Load** ‚Äî Store structured data in SQLite

Core modules:

* `data_processing/extract.py` ‚Äî Handles API requests with rate limits
* `data_processing/transform.py` ‚Äî Normalizes nested JSON structures
* `data_processing/load.py` ‚Äî Manages database transactions
* `app.py` ‚Äî FastAPI application
* `main.py` ‚Äî Orchestrates ETL pipeline execution

---

## üß† Assumptions

During the design and development of this ETL pipeline, a few key assumptions were made to balance **complexity**, **performance**, and **clarity** of the database schema.

1. **Simplified Relational Model**
   The database schema was initially designed with fewer tables to keep the structure lightweight and easily understandable.
   The assumption was that only the **most relevant** Pok√©mon data (name, stats, types, abilities, evolution info) would be required for analytics and API usage.

2. **Incremental Refinement**
   As the ETL logic evolved and more nested JSON structures were discovered in the Pok√©API, the schema was amended to introduce **normalized relationships** ‚Äî such as `pokemon_types`, `pokemon_abilities`, and `pokemon_stats`.
   This was a deliberate trade-off: keeping the schema simple at first, then expanding it only when necessary for data integrity and scalability.

3. **Pok√©API Data Consistency**
   It was assumed that the Pok√©API data is **complete and consistent**, meaning no Pok√©mon entries would be missing required fields (types, stats, abilities).
   The ETL logic still includes validation and logging in case the API returns incomplete or malformed records.

4. **Limited Dataset Scope**
   The ETL currently fetches a **subset of Pok√©mon** (e.g., first 12) for demonstration purposes.
   This assumption makes testing and iteration faster while retaining functional completeness.
   The pipeline can easily scale to process all Pok√©mon if needed.

5. **Local Environment Execution**
   The project assumes a **local or Dockerized setup** where SQLite and FastAPI are deployed on the same container.
   This simplifies configuration and removes the need for separate database hosting or networking layers.

---

## üß© Testing Strategy

Testing was implemented using Python‚Äôs built-in `unittest` framework to ensure each layer of the project is reliable and behaves as expected.

### ‚úÖ Unit Tests

* **`test_extract.py`** ‚Äî Validates Pok√©API response structure and rate limiting
* **`test_transform.py`** ‚Äî Checks that data normalization and mapping produce valid relational records
* **`test_load.py`** ‚Äî Ensures database inserts, updates, and transactions behave correctly
* **`test_main.py`** ‚Äî Confirms that the full ETL pipeline runs end-to-end without data loss

### üß™ Running Tests

```bash
python3 -m unittest discover -v
```

---

## ü§ù Contributing

1. Fork the repo
2. Create a new branch (`git checkout -b feature/new-feature`)
3. Commit changes (`git commit -m "Add new feature"`)
4. Push to your branch (`git push origin feature/new-feature`)
5. Submit a Pull Request

---

## ü§ñ Potential Improvements & Future Enhancements

### 1. **AI-Powered Data Analysis**

Integrating **AI and Machine Learning** models could unlock new capabilities such as:

* **Pok√©mon Stat Prediction**
* **Type Effectiveness Modeling**
* **Evolution Chain Prediction**
* **AI-Powered Recommendations**

### 2. **AI-Assisted Data Cleaning & Enrichment**

* Use **LLMs** or rule-based AI pipelines to automatically validate and enrich API data.

### 3. **Predictive Analytics Dashboard**

* Integrate an **AI insights dashboard** where users can visualize stats and run AI-based predictions.

### 4. **Data Pipeline Improvements**

* Add **Prefect** or **Airflow** for ETL orchestration
* Implement **async ETL execution**
* Introduce **data versioning**
* Include **AI-powered anomaly detection**

### 5. **Scalability & Infrastructure**

* Use **Docker Compose** or **Kubernetes**
* Move to **PostgreSQL** or **AWS RDS**
* Implement **Redis caching**
* Implement **GraphQL** API layer

### 6. **Developer Experience & Automation**

* GitHub Actions CI/CD
* AI code review tools
* AI-assisted documentation generation

### 7. **User-Facing Enhancements**

* Interactive Web UI (React + FastAPI)
* Search & Recommendation Engine
* Gamification Layer

### 8. **AI-Powered Monitoring & Maintenance**

* Anomaly detection in logs
* AI-based ETL run reports
* Natural language query support in API

---
**Made with ‚ù§Ô∏è for Pok√©mon fans and data engineers**
*Powered by [Pok√©API](https://pokeapi.co/) ‚Ä¢ [FastAPI](https://fastapi.tiangolo.com/) ‚Ä¢ [SQLite](https://www.sqlite.org/) ‚Ä¢ [Docker](https://www.docker.com/)*
